{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "678358a7-2a8b-4b4a-85bd-2965bdb2bf24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Distributed Grid Search - 2,800 Hyperparameter Combinations Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18476f7d-af4e-4600-adcb-2e82cb047c65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The cluster was unhealthy or has been terminated. Please restart the cluster or attach this notebook to a different cluster.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import time, json, os, hashlib\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "class GridSearchCheckpoint:\n",
    "    def __init__(self, experiment_name: str):\n",
    "        self.client = MlflowClient()\n",
    "        \n",
    "        # Check MLflow configuration\n",
    "        tracking_uri = mlflow.get_tracking_uri()\n",
    "        print(f\"MLflow tracking URI: {tracking_uri}\")\n",
    "        \n",
    "        # Safely create or get experiment\n",
    "        try:\n",
    "            # set_experiment creates experiment if not exists and returns experiment\n",
    "            mlflow.set_experiment(experiment_name)\n",
    "            self.experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "            \n",
    "            # Verify experiment was created/found\n",
    "            if self.experiment is None:\n",
    "                raise ValueError(f\"Failed to create/find experiment: {experiment_name}\")\n",
    "                \n",
    "            print(f\"MLflow experiment ready: {experiment_name} (ID: {self.experiment.experiment_id})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"MLflow experiment setup failed: {str(e)}\")\n",
    "            print(\"Attempting to create experiment directly...\")\n",
    "            \n",
    "            try:\n",
    "                # Try creating experiment directly with client\n",
    "                experiment_id = self.client.create_experiment(experiment_name)\n",
    "                self.experiment = self.client.get_experiment(experiment_id)\n",
    "            except Exception as create_error:\n",
    "                print(f\"Direct experiment creation failed: {str(create_error)}\")\n",
    "                # Try to get existing experiment by searching\n",
    "                experiments = self.client.search_experiments()\n",
    "                matching_exp = next((exp for exp in experiments if exp.name == experiment_name), None)\n",
    "                \n",
    "                if matching_exp:\n",
    "                    self.experiment = matching_exp\n",
    "                    mlflow.set_experiment(experiment_name)\n",
    "                else:\n",
    "                    raise SystemExit(f\"Cannot initialize MLflow experiment '{experiment_name}'. Please check MLflow configuration.\")\n",
    "        \n",
    "        self._cache, self._cache_time = {}, 0\n",
    "    \n",
    "    def get_hash(self, params: Dict) -> str:\n",
    "        return hashlib.md5(json.dumps(params, sort_keys=True).encode()).hexdigest()[:8]\n",
    "    \n",
    "    def get_completed(self) -> Dict[str, float]:\n",
    "        if time.time() - self._cache_time < 300: return self._cache\n",
    "        runs = self.client.search_runs(\n",
    "            experiment_ids=[self.experiment.experiment_id],\n",
    "            filter_string=\"status = 'FINISHED' and metrics.r2_score > 0\", max_results=5000)\n",
    "        self._cache = {run.data.tags.get('param_hash', ''): run.data.metrics.get('r2_score', 0.0) \n",
    "                      for run in runs if 'param_hash' in run.data.tags}\n",
    "        self._cache_time = time.time()\n",
    "        return self._cache\n",
    "    \n",
    "    def log_result(self, params: Dict, r2_score: float, exec_time: float):\n",
    "        with mlflow.start_run(run_name=f\"gs_{self.get_hash(params)}\"):\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_metrics({\"r2_score\": r2_score, \"execution_time\": exec_time})\n",
    "            mlflow.set_tags({\"param_hash\": self.get_hash(params), \"job\": \"grid_search\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df4bfe23-08eb-45d8-ade6-9e9c10b2f718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The cluster was unhealthy or has been terminated. Please restart the cluster or attach this notebook to a different cluster.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MLflow Debug Function\n",
    "def check_mlflow_status():\n",
    "    try:\n",
    "        import mlflow\n",
    "        print(f\"MLflow version: {mlflow.__version__}\")\n",
    "        print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "        \n",
    "        # Test basic MLflow operations\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        experiments = client.search_experiments(max_results=3)\n",
    "        print(f\"Accessible experiments: {len(experiments)}\")\n",
    "        for exp in experiments[:3]:\n",
    "            print(f\"  - {exp.name} (ID: {exp.experiment_id})\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"MLflow check failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Uncomment to run MLflow diagnostic\n",
    "# check_mlflow_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f4714f-c64c-418d-8356-41cac3ede57b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The cluster was unhealthy or has been terminated. Please restart the cluster or attach this notebook to a different cluster.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Distributed_RandomForest_GridSearch\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create checkpoint only if not exists\n",
    "# Replace the path with your own path\n",
    "if 'checkpoint' not in globals() or checkpoint is None:\n",
    "    checkpoint = GridSearchCheckpoint(f\"/Users/user/RF_GridSearch_{datetime.now().strftime('%Y%m%d')}\")\n",
    "    print(\"New checkpoint created\")\n",
    "else:\n",
    "    print(\"Using existing checkpoint (cache preserved)\")\n",
    "\n",
    "optimal_parallelism = min(max(1, spark.sparkContext.defaultParallelism // 4), 5)\n",
    "print(f\"System initialization complete (parallelism: {optimal_parallelism})\")\n",
    "\n",
    "# Check table existence\n",
    "table_name = \"df_selected_05\"\n",
    "try:\n",
    "    # Check if table exists in catalog\n",
    "    available_tables = [table.name for table in spark.catalog.listTables()]\n",
    "    if table_name not in available_tables:\n",
    "        raise ValueError(f\"Table '{table_name}' does not exist\")\n",
    "    \n",
    "    # Load data\n",
    "    df = spark.table(table_name).cache()\n",
    "    row_count = df.count()\n",
    "    col_count = len(df.columns)\n",
    "    print(f\"Data loaded successfully: {row_count:,} rows, {col_count} columns\")\n",
    "    \n",
    "    # Check required columns\n",
    "    if \"SalePrice\" not in df.columns:\n",
    "        raise ValueError(\"Required column 'SalePrice' not found in table\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Data loading failed: {str(e)}\")\n",
    "    print(f\"Available tables: {available_tables[:5]}\" if 'available_tables' in locals() else \"Failed to retrieve table list\")\n",
    "    print(\"Solutions:\")\n",
    "    print(f\"1. Verify table '{table_name}' is uploaded to Databricks\")\n",
    "    print(\"2. Check table name spelling\")\n",
    "    print(\"3. Verify table access permissions\")\n",
    "    raise SystemExit(\"Cannot proceed with grid search without actual data\")\n",
    "\n",
    "target_col = \"SalePrice\"\n",
    "df_vectorized = VectorAssembler(inputCols=[c for c in df.columns if c not in [target_col, \"id\"]], \n",
    "                               outputCol=\"features\", handleInvalid=\"skip\").transform(df).cache()\n",
    "print(\"Feature vectorization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "969a1c38-98ee-4584-807e-d85bbd5b9e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The cluster was unhealthy or has been terminated. Please restart the cluster or attach this notebook to a different cluster.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set Spark checkpoint directory before using .checkpoint()\n",
    "spark.sparkContext.setCheckpointDir(\"/dbfs/tmp/spark_checkpoints\")\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(RandomForestRegressor.numTrees, [500, 600, 646, 650, 700, 750, 800]) \\\n",
    "    .addGrid(RandomForestRegressor.maxDepth, [25, 35, 50, 100]) \\\n",
    "    .addGrid(RandomForestRegressor.minInstancesPerNode, [2, 3, 4, 5, 6]) \\\n",
    "    .addGrid(RandomForestRegressor.subsamplingRate, [0.65, 0.69, 0.7, 0.8, 0.9]) \\\n",
    "    .addGrid(RandomForestRegressor.featureSubsetStrategy, [\"0.3\", \"0.5\", \"0.7\", \"sqrt\"]) \\\n",
    "    .build()\n",
    "\n",
    "completed = checkpoint.get_completed()\n",
    "remaining_params = [p for p in param_grid \n",
    "                   if checkpoint.get_hash({param.name: value for param, value in p.items()}) not in completed]\n",
    "\n",
    "print(f\"Total {len(param_grid):,} combinations\")\n",
    "if len(completed) > 0:\n",
    "    print(f\"Restart: {len(param_grid) - len(remaining_params)} completed, {len(remaining_params)} remaining\")\n",
    "\n",
    "def run_grid_search(param_maps: List, data, target: str) -> Tuple[Dict, float]:\n",
    "    print(f\"Grid search started: {len(param_maps)} combinations\")\n",
    "    best_score, best_params = -1.0, {}\n",
    "    evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    \n",
    "    for i, param_map in enumerate(param_maps, 1):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            params_dict = {param.name: value for param, value in param_map.items()}\n",
    "            rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target, seed=42)\n",
    "            for param, value in param_map.items(): setattr(rf, param.name, value)\n",
    "            \n",
    "            cv = CrossValidator(estimator=rf, estimatorParamMaps=[param_map], evaluator=evaluator,\n",
    "                              numFolds=5, seed=42, parallelism=optimal_parallelism, collectSubModels=False)\n",
    "            \n",
    "            r2_score = cv.fit(data).avgMetrics[0]\n",
    "            checkpoint.log_result(params_dict, r2_score, time.time() - start_time)\n",
    "            \n",
    "            if r2_score > best_score: best_score, best_params = r2_score, params_dict.copy()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"{i}/{len(param_maps)} ({i/len(param_maps)*100:.1f}%) | Remaining time: {(elapsed * (len(param_maps) - i)) / 3600:.1f}h\")\n",
    "        except Exception as e:\n",
    "            print(f\"Combination {i} failed: {type(e).__name__}\")\n",
    "    \n",
    "    print(f\"Best performance: {best_score:.6f}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "def emergency_cleanup():\n",
    "    \"\"\"Emergency resource cleanup on interruption\"\"\"\n",
    "    try:\n",
    "        print(\"🧹 Clearing Spark caches...\")\n",
    "        spark.catalog.clearCache()\n",
    "        print(\"✅ Spark caches cleared\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Cache cleanup failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Save current progress to MLflow\n",
    "        if 'checkpoint' in globals() and hasattr(checkpoint, '_cache') and checkpoint._cache:\n",
    "            completed_count = len(checkpoint._cache)\n",
    "            print(f\"💾 Progress saved: {completed_count} combinations completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Progress save failed: {e}\")\n",
    "    \n",
    "    print(\"🔄 Resources cleaned up - safe to restart\")\n",
    "\n",
    "EXECUTE = False\n",
    "\n",
    "if EXECUTE:\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        print(\"🚀 Starting grid search with safety mechanisms...\")\n",
    "        df_vectorized.checkpoint()\n",
    "        best_params, best_score = run_grid_search(remaining_params, df_vectorized, target_col)\n",
    "        execution_time = datetime.now() - start_time\n",
    "        print(f\"Complete! Execution time: {execution_time}\")\n",
    "        print(f\"Best R2: {best_score:.6f}\")\n",
    "        \n",
    "        results_path = f\"/dbfs/mnt/results/grid_search_{start_time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
    "        with open(results_path, \"w\") as f:\n",
    "            json.dump({\"execution_time_seconds\": execution_time.total_seconds(),\n",
    "                      \"best_r2_score\": float(best_score), \"best_params\": best_params,\n",
    "                      \"completion_time\": datetime.now().isoformat()}, f, indent=2)\n",
    "        print(f\"Results saved: {results_path}\")\n",
    "        \n",
    "    except (KeyboardInterrupt, Exception) as e:\n",
    "        execution_time = datetime.now() - start_time\n",
    "        print(f\"\\n💥 Grid search terminated after {execution_time}\")\n",
    "        print(f\"Error: {type(e).__name__}: {str(e) if str(e) else 'User interruption'}\")\n",
    "        \n",
    "        # Save partial results\n",
    "        if 'checkpoint' in globals() and hasattr(checkpoint, '_cache'):\n",
    "            partial_results_path = f\"/dbfs/mnt/results/partial_grid_search_{start_time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(partial_results_path), exist_ok=True)\n",
    "                with open(partial_results_path, \"w\") as f:\n",
    "                    json.dump({\n",
    "                        \"execution_time_seconds\": execution_time.total_seconds(),\n",
    "                        \"completed_combinations\": len(checkpoint._cache),\n",
    "                        \"total_combinations\": len(param_grid),\n",
    "                        \"completion_percentage\": len(checkpoint._cache) / len(param_grid) * 100,\n",
    "                        \"termination_reason\": type(e).__name__,\n",
    "                        \"termination_time\": datetime.now().isoformat()\n",
    "                    }, f, indent=2)\n",
    "                print(f\"📊 Partial progress saved: {partial_results_path}\")\n",
    "            except Exception as save_error:\n",
    "                print(f\"⚠️ Failed to save partial results: {save_error}\")\n",
    "        \n",
    "        # Re-raise to maintain error behavior\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        # Always cleanup, regardless of success or failure\n",
    "        print(\"🧹 Final cleanup...\")\n",
    "        spark.catalog.clearCache()\n",
    "        print(\"Grid search complete - resources cleaned\")\n",
    "        spark.stop()\n",
    "        \n",
    "else:\n",
    "    print(\"Execution disabled: Set EXECUTE = True to start\")\n",
    "    print(\"Data loaded and ready for analysis (Spark session remains active)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is a distributed grid search for hyperparameter tuning of a random forest model. \n",
    "This code ran on Databricks using Spark and Hadoop engine along with MLflow for tracking and logging.\n",
    "This code generated models with the metrics of r2 surrounding 0.82 which is a lot lower than previous random search.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_distributed_grid_search",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
